import os
import json
from flask import Flask, request, jsonify
from flask_cors import CORS
from dotenv import load_dotenv
import google.generativeai as genai

# Carrega vari√°veis de ambiente
load_dotenv()

app = Flask(__name__)
CORS(app)

# --- CONFIGURA√á√ÉO GEMINI ---
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    print("\n‚ö†Ô∏è  ERRO CR√çTICO: GEMINI_API_KEY n√£o encontrada!")
else:
    genai.configure(api_key=GEMINI_API_KEY)
    print("‚úÖ Chave carregada com sucesso.")

def call_gemini(system_instruction, user_prompt):
    """
    Fun√ß√£o robusta: Tenta usar o modelo Flash, se falhar, tenta o Pro.
    """
    if not GEMINI_API_KEY:
        return {"error": "Chave de API n√£o configurada."}

    # Lista de modelos para tentar (na ordem de prioridade)
    models_to_try = ["gemini-1.5-flash", "gemini-pro", "gemini-2.5-flash"]
    
    last_error = None

    for model_name in models_to_try:
        try:
            print(f"üì° Tentando usar o modelo: {model_name}...")
            
            model = genai.GenerativeModel(
                model_name=model_name,
                system_instruction=system_instruction,
                generation_config={"response_mime_type": "application/json"}
            )

            response = model.generate_content(user_prompt)
            print(f"‚úÖ Sucesso com {model_name}!")
            
            return json.loads(response.text)

        except Exception as e:
            print(f"‚ö†Ô∏è Falha com {model_name}: {e}")
            last_error = e
            continue # Tenta o pr√≥ximo da lista
    
    print(f"‚ùå Todas as tentativas falharam. Erro final: {last_error}")
    return {"error": str(last_error)}

# --- ROTA 1: GERAR HIST√ìRIA (Prompt Rico Restaurado) ---
@app.route('/api/generate-story', methods=['POST'])
def generate_story():
    data = request.json
    idea = data.get('idea', '')
    user = data.get('user', '')
    action = data.get('action', '')
    benefit = data.get('benefit', '')

    system = """
    Voc√™ √© um Product Owner experiente (PO Coach). 
    Sua tarefa √© criar ou refinar uma User Story baseada em inputs parciais.

    INSTRU√á√ÉO DE REFINAMENTO:
    Analise o rascunho gerado a partir da ideia e dos inputs. 
    Se o verbo da a√ß√£o for passivo (ex: 'visualizar', 'ver') ou o valor for vago (ex: 'facilitar', 'melhorar'),
    sugira um refinamento no verbo e na frase 'para que' para torn√°-los mais acion√°veis, espec√≠ficos e orientados a valor.

    O objetivo √© gerar o rascunho can√¥nico de alta qualidade: "Como um [Persona], eu quero [A√ß√£o], para que [Valor]."

    Retorne APENAS um JSON no formato:
    {
        "user": "string (quem √© o usu√°rio)",
        "action": "string (o que ele quer fazer - refinado se necess√°rio)",
        "benefit": "string (para que - refinado se necess√°rio)",
        "formattedStory": "string (HTML formatado com tags <strong> para Como, Quero, Para)"
    }
    Se algum campo estiver vazio, infira o melhor conte√∫do baseado no campo 'idea', aplicando as regras de refinamento acima.
    """
    
    prompt = f"""
    Ideia Bruta: {idea}
    Usu√°rio sugerido: {user}
    A√ß√£o sugerida: {action}
    Benef√≠cio sugerido: {benefit}
    """

    result = call_gemini(system, prompt)
    if result and "error" in result: return jsonify(result), 400
    if result: return jsonify(result)
    return jsonify({"error": "Falha na IA"}), 500

# --- ROTA 2: PERGUNTAS ---
@app.route('/api/generate-questions', methods=['POST'])
def generate_questions():
    data = request.json
    context = data.get('context', '')
    system = """
    Atue como um Analista de Neg√≥cios S√™nior e Especialista em QA.
    
    Sua miss√£o √© analisar o Contexto da Hist√≥ria de Usu√°rio e identificar lacunas, riscos e regras n√£o expl√≠citas.
    Gere perguntas estrat√©gicas para "blindar" essa funcionalidade tente identificar pontos que n√£o foram mencionados anteriormente.

    Retorne JSON: { "questions": [{"label": "...", "ph": "..."}] }
    """
    result = call_gemini(system, f"Contexto: {context}")
    if result and "error" in result: return jsonify(result), 400
    if result: return jsonify(result['questions'])
    return jsonify({"error": "Falha na IA"}), 500

# --- ROTA CHAT ---
@app.route('/api/chat-rules', methods=['POST'])
def chat_rules():
    data = request.json
    context = data.get('context', '')
    user_message = data.get('message', '')
    
    system = """
    Voc√™ √© um PO Coach. O usu√°rio est√° adicionando regras manualmente.
    Reconhe√ßa a regra e pergunte se h√° mais.
    Retorne JSON: { "reply": "..." }
    """
    
    prompt = f"Contexto: {context}\nUsu√°rio disse: {user_message}"
    result = call_gemini(system, prompt)
    if result and "error" in result: return jsonify(result), 400
    if result: return jsonify(result)
    return jsonify({"error": "Falha na IA"}), 500

# --- ROTA 2b: CONSOLIDAR ---
@app.route('/api/consolidate-rules', methods=['POST'])
def consolidate_rules():
    data = request.json
    context = data.get('context', '')
    qa_pairs = data.get('qaPairs', [])
    chat_history = data.get('chatHistory', [])
    
    qa_text = "\n".join([f"P: {i['question']} R: {i['answer']}" for i in qa_pairs])
    chat_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in chat_history])
    
    system = """
    Converta tudo (Quiz + Chat) em Regras Formais.
    Retorne JSON: { "rules": [{"id": "RN-01", "text": "..."}] }
    """
    result = call_gemini(system, f"Hist√≥ria: {context}\nQuiz: {qa_text}\nChat: {chat_text}")
    if result and "error" in result: return jsonify(result), 400
    if result: return jsonify(result['rules'])
    return jsonify({"error": "Falha na IA"}), 500

# --- ROTA 3: GHERKIN ---
@app.route('/api/generate-gherkin', methods=['POST'])
def generate_gherkin():
    data = request.json
    rules = data.get('rules', [])
    system = """
    QA BDD. Crie cen√°rios Gherkin. Use tags HTML <span> com classes .gherkin-keyword, .gherkin-variable.
    Retorne JSON: { "scenarios": [{"ruleId": "...", "originalRule": "...", "gherkinText": "..."}] }
    """
    result = call_gemini(system, f"Regras: {json.dumps(rules)}")
    if result and "error" in result: return jsonify(result), 400
    if result: return jsonify(result['scenarios'])
    return jsonify({"error": "Falha na IA"}), 500

# --- ROTA 4: VALIDAR ---
@app.route('/api/validate-story', methods=['POST'])
def validate_story():
    data = request.json
    story = data.get('story', '')
    rules = data.get('rules', [])
    scenarios = data.get('scenarios', [])
    system = """
    Agile Coach. Avalie (0-100) e sugira splitting se necessario.
    Retorne JSON: 
    { "score": int, "message": "str", "isLarge": bool, "splittingSuggestions": [{"type": "...", "title": "...", "description": "..."}] }
    """
    prompt = f"Story: {story}\nRules: {len(rules)}\nScenarios: {len(scenarios)}"
    result = call_gemini(system, prompt)
    if result and "error" in result: return jsonify(result), 400
    if result: return jsonify(result)
    return jsonify({"error": "Falha na IA"}), 500

if __name__ == '__main__':
    print("üöÄ Servidor Story-4D rodando em http://localhost:5000")
    app.run(debug=True, port=5000)
